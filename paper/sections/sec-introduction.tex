% !TEX root = ../convolutional_w2.tex

\section{Introduction}

Probability distributions are ubiquitous objects in computer graphics, used to encapsulate possibly uncertain information associated with arbitrary geometric domains. Examples include image histograms, geometric features, relaxations of correspondence maps, and even physical quantities like BRDFs.
%\fernando{remove feature histograms to avoid repeating feature}% ok
To compare these objects, it is important to define an adequate notion of proximity or coverage quantifying the discrepancy or, equivalently, similarity between distributions. These computations are commonly posed and analyzed within the theory of \emph{optimal transportation}.

The prototypical problem in optimal transportation is the evaluation of Wasserstein (also known as Earth Mover's) distances between distributions~\cite{villani-2003,rubner-2000}. These distances quantify the geometric discrepancy between two distributions %someone removed "between distributions" but then it doesn't parse grammatically... also no comma here please!
by measuring the minimal amount of ``work'' needed to move all the mass contained in one distribution onto the other.
Recent developments show that incorporating these distances into optimization objectives yields powerful tools for manipulating distributions for tasks like density interpolation, barycenter computation, and correspondence estimation. As a simple example, suppose we are given two delta functions $\delta_x,\delta_y$ centered at $x,y \in \R^2$. While the Euclidean average $\nicefrac{(\delta_x+\delta_y)}{2}$ is bimodal at $x$ and $y$, solving for the distribution that minimizes the sum of squared two-Wasserstein distances to $\delta_x$ and $\delta_y$ is a Dirac at the midpoint $\nicefrac{(x+y)}{2}$, thus offering a geometric notion of the midpoint of two distributions.
%n alternative \emph{displacement} interpolation between distributions.

A limiting factor in optimal transportation is the complexity of the underlying minimization problem. The usual linear program describing optimal transportation is related to minimum-cost matching, with a quadratic number of variables and time complexity scaling at least cubically in the size of the domain~\cite{burkard1999linear}.
%worst-case behavior that scales polynomially to the number of samples~\cite{}.
This poor complexity is largely due to the use of coupling variables representing the amount of mass transported between every pair of samples. %, with pairwise distance costs.
Hence, existing large-scale methods often resort to aggressive or ad-hoc approximations that can lose connections to transportation theory or compensate with alternative formulations that apply only to restricted cases.
 % These modifications can accentuate unattractive artifacts associated with nearly ill-posed transportation problems.%\marco{should we dare to put some references here? The Pelé Werman 2009 ICCV paper could be one, maybe we should tone down a bit the rhetoric in order to incorporate references}. %\justin{Some references here are alright if the authors explicitly discuss the matter.  I looked up the Pele/Werman paper and didn't see much of that, however!  Any other papers?

% MC: Ok, I see you are explicitly referring to the "quadratic" part in your sentence. In that case I agree with you, there's no obvious reference to my knowledge, apart from the widely used quantization trick, i.e. pool all points together and use k-means to produce histograms.

%\marco{I added again the mention "a number of variables that" that had been removed, because saying that "a LP scales as ..." is ambiguous: the scaling might apply to the number of constraints (which is indeed the case in the dual form), the number of variables, the computational effort etc... I think we might want to be more clear here. Besides we do not want the reader to feel that anything is quadratic in terms of complexity. I also added the mention $O(n^6)$ to insist that this has obviously a huge impact.}
%\fernando{I avoided to go to deep into the right complexity of LP. We could instead cite some LP analysis paper.}

This paper introduces a fast, scalable numerical framework for optimal transportation over geometric domains.  Our work draws insight from recent advances in machine learning approximating optimal transportation distances using entropic regularization~\cite{cuturi-2013}.  
%\gabriel{I have changed the statement ``By extending their model to continuous domains''. Please check/correct if needed. }
We adapt this approach to continuous domains using faithful finite elements discretizations of the corresponding optimization problems. This yields a novel approach to optimal transportation without computing or storing pairwise distances on arbitrary shapes.

After discretization, our algorithm for approximating Wasserstein distances becomes a simple iterative scheme with linear convergence, whose iterations require convolution of vectors against discrete diffusion kernels---hence the name \emph{convolutional Wasserstein distance}.  We also leverage our framework to design methods for interpolation between distributions, computation of weighted barycenters of sets of distributions, and more complex distribution-valued correspondence problems. Each of these problems is solved with straightforward iterative methods scaling linearly in the size of the data and domain.  We demonstrate the versatility of our methods with examples in image processing, shape analysis, and BRDF interpolation.
%justin removed "efficiency" since that's a bit debatable...

% Several computations in graphics take place over a geometric domain, incorporating notions of distance to inform algorithmic decisions.  For example, photography and imaging algorithms generally combine information from pixels or voxels nearby on the image plane or in intensity.  Similarly, geodesic distances, heat kernels, and related quantities figure into many techniques for shape analysis and editing.  Many of these techniques can be cast as optimization problems whose objectives contain distance-based terms, e.g.\ optimizing measures of proximity or coverage.  This structure leads to computational challenges associated with optimization over graphics-scale domains. \marco{I feel the next sentence is a bit open ended, and delays a little too much the crux of the paper, that starts in the next paragraph} Objectives involving pairwise distances quickly become infeasible:  Simply evaluating these distances is a challenge on curved surfaces, and the resulting storage or algorithmic complexity scales quadratically in the size of the domain.

% A natural approach to geometric optimization in graphics relies upon probability densities.  Densities provide a relaxed, general approach to modeling geometric features, encapsulating points and curves as concentrated measures as well as uncertain notions of position and superposition.  Furthermore, physical quantities, like BRDFs for lighting, that exhibit positivity and conservation laws often are expressible as densities.  Discretely, densities can be modeled as nonnegative functions that integrate to one, a convex set easily expressed using finite elements and other discretization tools.

% Modeling using densities requires some adjustment in technical approach.  For instance, the midpoint of $x,y\in\R^2$ is $(x+y)/2$, but if $\delta_x$ and $\delta_y$ are densities centered at $x$ and $y$, the algebraic average $(\delta_x+\delta_y)/2$ will be bimodal at $x$ and $y$ rather than centered at the midpoint.  This behavior is undesirable in many circumstances:  An intuitive average of two single-highlight BRDFs likely should not yield a BRDF with two highlights.    The \emph{de facto} standard model addressing such issues is \emph{optimal transportation}, the optimization problem associated with the Earth Mover's or Wasserstein distance between probability distributions~\cite{villani-2003,rubner-2000}. Incorporating these distances into geometric optimization problems yields desirable geometric behavior for problems like interpolation, barycenter computation, correspondence, and so on.

% A limiting factor of optimal transportation for graphics is the complexity of the underlying minimization problem.  The linear program describing optimal transportation has a number of variables that scales quadratically with the size of the domain due to a pairwise distance term.  Hence, methods in this field often resort to aggressive or ad-hoc approximations that can lose connections to transportation theory, or compensate with better-scaling differential formulations that apply only to restricted cases.  These modifications often accentuate unattractive artifacts associated with nearly ill-posed transportation problems, which can accentuate errors or irregularities in the input data\marco{should we dare to put some references here? The Pelé Werman 2009 ICCV paper could be one, maybe we should tone down a bit the rhetoric in order to incorporate references}.

% This paper introduces a practical and remarkably simple framework for optimal transportation over geometric domains considered in graphics applications.  The key theoretical insight is that an entropic regularizer for the transportation problem recently introduced in the machine learning literature for discrete problems in which pairwise distances can be computed and stored~\cite{cuturi-2013} actually can be applied \emph{without} ever computing or storing these distances, on images, volumes, triangle meshes, and other domains admitting Laplacian or diffusion operators.  Thanks to the connection to heat diffusion and Gaussian convolution, we call our regularized distances \emph{convolutional Wasserstein distances}.

% The resulting algorithms are implementable in a few lines of code and exhibit unconditional convergence at rates suitable for graphics tools.  They serve dual roles of making large-scale transportation feasible for graphics problems and smoothing potential artifacts in the result.  The regularized version of transport can be applied equally well to transportation with quadratic and linear ground distances, generalizing more specific methods like~\cite{solomon-2014} that apply to a single ground distance and domain.  Furthermore, they can solve problems that are orders of magnitude larger than those approached by previous techniques in the same class.

% Complementing problems whose challenge is to evaluate transportation distances, we provide several examples of optimizations \emph{over} transportation distances that can be solved efficiently in our framework.  We show how to find the barycenter of a set of distributions, how to interpolate from one distribution to another, and how to solve more complex ``soft'' mapping problems.  Each of these problems can be solved with straightforward iterative methods that scale linearly in the size of the data and domain.  We also suggest a slight modification of the basic algorithms to counteract diffusive behavior that can result from the regularization. 